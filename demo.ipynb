{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchLOB: Safe RL & FIFO Matching Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **FIFO (Non-RL) Execution**: A baseline passive strategy relying on the orderbook's price-time priority.\n",
    "2. **Safe RL Execution**: A PPO agent using **PID-Lagrangian** control to minimize slippage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from torch_exchange.environment import TorchExecutionEnv\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ M1/M2/M3 GPU Detected (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✅ M1/M2/M3 GPU Detected (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FIFO Execution (Non-RL Baseline)\n",
    "This strategy simply places Passive Orders (at the touch) and waits. It utilizes the First-In-First-Out (Price-Time) matching engine logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FIFO Baseline...\n",
      "FIFO Result: Steps=100, Avg Cost (Slippage)=0.0000\n"
     ]
    }
   ],
   "source": [
    "from torch_exchange.environment import TorchExecutionEnv\n",
    "\n",
    "env_fifo = TorchExecutionEnv(task='sell', task_size=5000, device=device, book_depth=20)\n",
    "obs = env_fifo.reset()\n",
    "\n",
    "print(\"Running FIFO Baseline...\")\n",
    "total_reward = 0\n",
    "total_cost = 0\n",
    "steps = 0\n",
    "\n",
    "# Simple policy: Always place small passive orders\n",
    "# Action space: [FT, M, NT, PP]\n",
    "# FIFO/Passive Strategy: 100% at Passive Price (PP) or Near Touch (NT)\n",
    "# Let's say we split between NT and PP\n",
    "\n",
    "fifo_action = np.array([0, 0, 50, 50], dtype=np.int32)\n",
    "\n",
    "while True:\n",
    "    obs, reward, done, info = env_fifo.step(fifo_action)\n",
    "    total_reward += reward\n",
    "    total_cost += info.get('cost', 0)\n",
    "    steps += 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"FIFO Result: Steps={steps}, Avg Cost (Slippage)={total_cost/steps:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Safe RL Training (PID-Lagrangian)\n",
    "Training PPO with a **PID-Lagrangian** constraint to keep slippage low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Safe RL Training (PID-Lagrangian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1024it [01:37, 10.47it/s, batch_rew=-0.30, batch_cost=59.38, loss=6.427, lambda=0.0000]                    \n"
     ]
    }
   ],
   "source": [
    "from torch_exchange.ppo import PPOAgent\n",
    "\n",
    "env_rl = TorchExecutionEnv(task='sell', task_size=5000, device=device, book_depth=20)\n",
    "\n",
    "# Initialize with 'pid' safety mode and tight cost limit\n",
    "agent = PPOAgent(\n",
    "    env_rl, \n",
    "    device=device, \n",
    "    lr = 3e-5,\n",
    "    cost_limit=0.5, # Limit slippage\n",
    "    pid_Kp=2.0\n",
    ")\n",
    "\n",
    "print(\"Starting Safe RL Training (PID-Lagrangian)...\")\n",
    "agent.train(total_timesteps=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
